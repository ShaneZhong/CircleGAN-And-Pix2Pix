{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CycleGAN Van Gogh Painting to Photo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaneZhong/CircleGAN-And-Pix2Pix/blob/master/CycleGAN_Van_Gogh_Painting_to_Photo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAPgU4PXWsMj",
        "colab_type": "text"
      },
      "source": [
        "# CircleGAN vangogh Painting to Photo\n",
        "\n",
        "Update Date: 10 July 2019\n",
        "\n",
        "Reference:\n",
        "* https://www.tensorflow.org/datasets/datasets#cycle_gan\n",
        "* https://towardsdatascience.com/cyclegans-and-pix2pix-5e6a5f0159c4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v1CUZ0dkOo_F"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "qmkj-80IHxnd",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIvH_PpG9LK5",
        "colab_type": "text"
      },
      "source": [
        "## Connect to your Drive to save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32HNeSD-9NkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_xnMOsbqHz61"
      },
      "source": [
        "# CycleGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ds4o1h4WHz9U"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/beta/tutorials/generative/cyclegan\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/generative/cyclegan.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/generative/cyclegan.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/r2/tutorials/generative/cyclegan.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ITZuApL56Mny"
      },
      "source": [
        "This notebook demonstrates unpaired image to image translation using conditional GAN's, as described in [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593), also known as CycleGAN. The paper proposes a method through which we can capture the characteristics of one image domain and figure out how these characteristics could be translated into another image domain, all in the absence of any paired training examples. \n",
        "\n",
        "This notebook assumes you are familiar with Pix2Pix, which you can learn about in the [Pix2Pix tutorial](https://www.tensorflow.org/beta/tutorials/generative/pix2pix). The code for CycleGAN is similar, the main difference is an additional loss function, and the use of unpaired training data.\n",
        "\n",
        "CycleGAN uses a cycle consistency loss to enable training without the need for paired data. In other words, it can translate from one domain to another without a one-to-one mapping between the source and target domain. \n",
        "\n",
        "This opens up the possibility to do a lot of interesting tasks like photo-enhancement, image colorization, style transfer, etc. All you need is the source and the target dataset (which is simply a directory of images)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e1_Y75QXJS6h"
      },
      "source": [
        "## Set up the input pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5fGHWOKPX4ta"
      },
      "source": [
        "Install the [tensorflow_examples](https://github.com/tensorflow/examples) package that enables importing of the generator and the discriminator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bJ1ROiQxJ-vY",
        "colab": {}
      },
      "source": [
        "# load the github with pre-defined generator and discriminator\n",
        "!pip install git+https://github.com/tensorflow/examples.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lhSsUx9Nyb3t",
        "colab": {}
      },
      "source": [
        "# using tensorflow 2.0\n",
        "!pip install tensorflow-gpu==2.0.0-beta1\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YfIk2es3hJEd",
        "colab": {}
      },
      "source": [
        "# loading tf 2.0 features\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# loading the dataset and pix2pix from github\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow_examples.models.pix2pix import pix2pix\n",
        "\n",
        "# other libraries\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Disable progress bar\n",
        "# https://www.tensorflow.org/datasets/api_docs/python/tfds/disable_progress_bar\n",
        "# https://www.tensorflow.org/beta/guide/data_performance\n",
        "# autotone decouple the computation of CPU and GPU. Making the pipeline\n",
        "#   process a lot faster.\n",
        "tfds.disable_progress_bar()\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iYn4MdZnKCey"
      },
      "source": [
        "## Input Pipeline\n",
        "\n",
        "This tutorial trains a model to translate from images of vangogh's paintings, to photos. You can find this dataset and similar ones [here](https://www.tensorflow.org/datasets/datasets#cycle_gan). \n",
        "\n",
        "As mentioned in the [paper](https://arxiv.org/abs/1703.10593), apply random jittering and mirroring to the training dataset. These are some of the image augmentation techniques that avoids overfitting.\n",
        "\n",
        "This is similar to what was done in [pix2pix](https://www.tensorflow.org/beta/tutorials/generative/pix2pix#load_the_dataset)\n",
        "* In random jittering, the image is resized to `286 x 286` and then randomly cropped to `256 x 256`.\n",
        "* In random mirroring, the image is randomly flipped horizontally i.e left to right."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yww0zObOjG43",
        "colab_type": "text"
      },
      "source": [
        "### Pipeline:\n",
        "\n",
        "Without pipelining, the CPU and the GPU/TPU sit idle much of the time:\n",
        "![without Pipeline](https://www.tensorflow.org/images/datasets_without_pipelining.png)\n",
        "\n",
        "\n",
        "With pipelining, idle time diminishes significantly:\n",
        "![with pipeline](https://www.tensorflow.org/images/datasets_with_pipelining.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iuGVPOo7Cce0",
        "colab": {}
      },
      "source": [
        "# https://www.tensorflow.org/datasets/datasets#cycle_gan\n",
        "dataset, metadata = tfds.load('cycle_gan/vangogh2photo',\n",
        "                              with_info=True, as_supervised=True)\n",
        "\n",
        "# train and test is using the same source\n",
        "# but later on the train/test dataset are created by\n",
        "# shuffle().batch() - i.e. randomly select ones from the same source\n",
        "# Both are already in 256*256*3 shape\n",
        "train_vangogh, train_photo = dataset['trainA'], dataset['trainB']\n",
        "test_vangogh, test_photo = dataset['testA'], dataset['testB']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LHP6QlDqbSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(train_vangogh)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2CbTEt448b4R",
        "colab": {}
      },
      "source": [
        "# Constant - using CAPTAL LETTERS\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/train/shuffle_batch\n",
        "# BUFFER_SIZE = Minimum number elements in the queue after a dequeue, \n",
        "#    used to ensure a level of mixing of elements.\n",
        "BUFFER_SIZE = 1000\n",
        "# BATCH_SIZE = The new batch size pulled from the queue.\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "# resize to 256*256\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Yn3IwqhiIszt",
        "colab": {}
      },
      "source": [
        "def random_crop(image):\n",
        "  # crop the image to 256*256*3\n",
        "  cropped_image = tf.image.random_crop(\n",
        "      image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "\n",
        "  return cropped_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "muhR2cgbLKWW",
        "colab": {}
      },
      "source": [
        "# normalizing the images to [-1, 1]\n",
        "def normalize(image):\n",
        "  # Convert the image to float 32\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  # 256/127.5 = 2, 0/127.5 = 0.\n",
        "  image = (image / 127.5) - 1\n",
        "  return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fVQOjcPVLrUc",
        "colab": {}
      },
      "source": [
        "def random_jitter(image):\n",
        "  # resizing to 286 x 286 x 3. Using nearest neighour to resize\n",
        "  image = tf.image.resize(image, [286, 286],\n",
        "                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "  # randomly cropping to 256 x 256 x 3\n",
        "  image = random_crop(image)\n",
        "\n",
        "  # random mirroring\n",
        "  image = tf.image.random_flip_left_right(image)\n",
        "\n",
        "  return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tyaP4hLJ8b4W",
        "colab": {}
      },
      "source": [
        "# to train image: resize, crop, mirror and normalise\n",
        "def preprocess_image_train(image, label):\n",
        "  image = random_jitter(image)\n",
        "  image = normalize(image)\n",
        "  return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VB3Z6D_zKSru",
        "colab": {}
      },
      "source": [
        "# to test image: normalise image.\n",
        "# since the test dataset is already in 256*256*3 size, no reshape required\n",
        "def preprocess_image_test(image, label):\n",
        "  image = normalize(image)\n",
        "  return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wYvAhEjqwOh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# map function - map(function, iterables)\n",
        "def myfunc(n):\n",
        "  return len(n)\n",
        "\n",
        "x = map(myfunc, ('apple', 'banana', 'cherry'))\n",
        "print(x)\n",
        "print(list(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RsajGXxd5JkZ",
        "colab": {}
      },
      "source": [
        "# Apply the procecess_image_train function to train_vangogh images\n",
        "# cache() to speed up the loading time\n",
        "# shuffle().batch():\n",
        "#     tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size).\n",
        "#     BUFFER_SIZE = min_after_dequeue\n",
        "#     https://www.tensorflow.org/api_docs/python/tf/train/shuffle_batch\n",
        "# AUTOTUNE parallels CPU and GPU\n",
        "\n",
        "train_vangogh = train_vangogh.map(\n",
        "    preprocess_image_train, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
        "    BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "train_photo = train_photo.map(\n",
        "    preprocess_image_train, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
        "    BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "# apply preprocess_image_test to test set\n",
        "test_vangogh = test_vangogh.map(\n",
        "    preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
        "    BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "test_photo = test_photo.map(\n",
        "    preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
        "    BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPbfouQ6uUuZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# net(iter()) - iteration after run next()\n",
        "lst = iter([1,2,3])\n",
        "print(next(lst))\n",
        "print(next(lst))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e3MhJ3zVLPan",
        "colab": {}
      },
      "source": [
        "# iterate one at a time\n",
        "sample_vangogh = next(iter(train_vangogh))\n",
        "sample_photo = next(iter(train_photo))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dL4kPPsvSDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(sample_vangogh)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUAchijMv2jb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_vangogh[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijOsgmpyx2Bg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalised photo\n",
        "sample_photo[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4pOYjMk_KfIB",
        "colab": {}
      },
      "source": [
        "# Since the Batch size = 1, only one paint can be visualised\n",
        "# i.e. sample_vangogh[0] is Ok, but sample_vangogh[1] is invalid\n",
        "plt.subplot(121)\n",
        "plt.title('vangogh')\n",
        "plt.imshow(sample_vangogh[0] * 0.5 + 0.5)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('vangogh with random jitter')\n",
        "plt.imshow(random_jitter(sample_vangogh[0]) * 0.5 + 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0KJyB9ENLb2y",
        "colab": {}
      },
      "source": [
        "plt.subplot(121)\n",
        "plt.title('Photo')\n",
        "plt.imshow(sample_photo[0] * 0.5 + 0.5)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('Photo with random jitter')\n",
        "plt.imshow(random_jitter(sample_photo[0]) * 0.5 + 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hvX8sKsfMaio"
      },
      "source": [
        "## Import and reuse the Pix2Pix models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cGrL73uCd-_M"
      },
      "source": [
        "Import the generator and the discriminator used in [Pix2Pix](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py) via the installed [tensorflow_examples](https://github.com/tensorflow/examples) package.\n",
        "\n",
        "The model architecture used in this tutorial is very similar to what was used in [pix2pix](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py). Some of the differences are:\n",
        "* Cyclegan uses [instance normalization](https://arxiv.org/abs/1607.08022) instead of [batch normalization](https://arxiv.org/abs/1502.03167).\n",
        "* The [CycleGAN paper](https://arxiv.org/abs/1703.10593) uses a modified `resnet` based generator. This tutorial is using a modified `unet` generator for simplicity.\n",
        "\n",
        "There are 2 generators (G and F) and 2 discriminators (X and Y) being trained here. \n",
        "* Generator `G` learns to transform image `X` to image `Y`. $(G: X -> Y)$\n",
        "* Generator `F` learns to transform image `Y` to image `X`. $(F: Y -> X)$\n",
        "* Discriminator `D_X` learns to differentiate between image `X` and generated image `X` (`F(Y)`).\n",
        "* Discriminator `D_Y` learns to differentiate between image `Y` and generated image `Y` (`G(X)`).\n",
        "\n",
        "![Cyclegan model](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/generative/images/cyclegan_model.png?raw=1)\n",
        "\n",
        "* X is vangogh painting\n",
        "* Y is photo\n",
        "* generator_g convert vangogh painting to photo\n",
        "* generator_f convert photo to vangogh painting\n",
        "* discriminator_x check if the input is a vangogh Painting\n",
        "* discriminator_y check if the input is a photo\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8ju9Wyw87MRW",
        "colab": {}
      },
      "source": [
        "# RGB channels\n",
        "OUTPUT_CHANNELS = 3\n",
        "\n",
        "# import unet generator structure from the github\n",
        "generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
        "generator_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
        "\n",
        "discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n",
        "discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "63puOsYT0mTS"
      },
      "source": [
        "[pix2pix script link](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py)\n",
        "```\n",
        "def unet_generator(output_channels, norm_type='batchnorm'):\n",
        "  \"\"\"Modified u-net generator model (https://arxiv.org/abs/1611.07004).\n",
        "  Args:\n",
        "    output_channels: Output channels\n",
        "    norm_type: Type of normalization. Either 'batchnorm' or 'instancenorm'.\n",
        "  Returns:\n",
        "    Generator model\n",
        "  \"\"\"\n",
        "  ```\n",
        "  \n",
        "  ----\n",
        "  ```\n",
        "def discriminator(norm_type='batchnorm', target=True):\n",
        "  \"\"\"PatchGan discriminator model (https://arxiv.org/abs/1611.07004).\n",
        "  Args:\n",
        "    norm_type: Type of normalization. Either 'batchnorm' or 'instancenorm'.\n",
        "    target: Bool, indicating whether target image is an input or not.\n",
        "  Returns:\n",
        "    Discriminator model\n",
        "    \n",
        "    \n",
        "  if target:\n",
        "    return tf.keras.Model(inputs=[inp, tar], outputs=last)\n",
        "  else:\n",
        "    return tf.keras.Model(inputs=inp, outputs=last)\n",
        "  \"\"\"\n",
        "  ```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wDaGZ3WpZUyw",
        "colab": {}
      },
      "source": [
        "# generator_g convert vangogh painting to photo\n",
        "# generator_f convert photo to vangogh painting\n",
        "to_photo = generator_g(sample_vangogh)\n",
        "to_vangogh = generator_f(sample_photo)\n",
        "plt.figure(figsize=(8, 8))\n",
        "contrast = 8\n",
        "\n",
        "imgs = [sample_vangogh, to_photo, sample_photo, to_vangogh]\n",
        "title = ['vangogh', 'To Photo', 'Photo', 'To vangogh']\n",
        "\n",
        "for i in range(len(imgs)):\n",
        "  plt.subplot(2, 2, i+1)\n",
        "  plt.title(title[i])\n",
        "  if i % 2 == 0:\n",
        "    plt.imshow(imgs[i][0] * 0.5 + 0.5)\n",
        "  else:\n",
        "    plt.imshow(imgs[i][0] * 0.5 * contrast + 0.5)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O5MhJmxyZiy9",
        "colab": {}
      },
      "source": [
        "# discriminator_x check if the input is a vangogh Painting\n",
        "# discriminator_y check if the input is a photo\n",
        "# PatchGAN is used in discriminators with 30*30 output\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.title('Is a real photo?')\n",
        "plt.imshow(discriminator_y(sample_photo)[0, ..., -1], cmap='RdBu_r')\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('Is a real vangogh painting?')\n",
        "plt.imshow(discriminator_x(sample_vangogh)[0, ..., -1], cmap='RdBu_r')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-IHJpnF1iZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# value ranges from -1 to 1\n",
        "discriminator_y(sample_photo)[0, ..., -1][28]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0FMYgY_mPfTi"
      },
      "source": [
        "## Loss functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JRqt02lupRn8"
      },
      "source": [
        "In CycleGAN, there is no paired data to train on, hence there is no guarantee that the input `x` and the target `y` pair are meaningful during training. Thus in order to enforce that the network learns the correct mapping, the authors propose the cycle consistency loss.\n",
        "\n",
        "The discriminator loss and the generator loss are similar to the ones used in [pix2pix](https://www.tensorflow.org/beta/tutorials/generative/pix2pix#define_the_loss_functions_and_the_optimizer)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cyhxTuvJyIHV",
        "colab": {}
      },
      "source": [
        "# multiply factor to the L1 loss (circle loss)\n",
        "LAMBDA = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q1Xbz5OaLj5C",
        "colab": {}
      },
      "source": [
        "# https://www.tensorflow.org/api_docs/python/tf/keras/losses/binary_crossentropy\n",
        "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wkMNfBWlT-PV",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(real, generated):\n",
        "  # if the image is real, disc should identify it as 1\n",
        "  real_loss = loss_obj(tf.ones_like(real), real)\n",
        "  \n",
        "  # if the image is generated, disc sould identify it as 0\n",
        "  generated_loss = loss_obj(tf.zeros_like(generated), generated)\n",
        "\n",
        "  # sum up\n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "  # the total loss divid by two\n",
        "  return total_disc_loss * 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "90BIcCKcDMxz",
        "colab": {}
      },
      "source": [
        "# for genetor, we mark it as sucessful if the generated is disc as 1\n",
        "# i.e. whether we successfuly fool the disc\n",
        "def generator_loss(generated):\n",
        "  return loss_obj(tf.ones_like(generated), generated)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5iIWQzVF7f9e"
      },
      "source": [
        "Cycle consistency means the result should be close to the original input. For example, if one translates a sentence from English to French, and then translates it back from French to English, then the resulting sentence should be the same as the  original sentence.\n",
        "\n",
        "In our case:\n",
        "* X is vangogh painting\n",
        "* Y is photo\n",
        "* generator_g convert vangogh painting to photo\n",
        "* generator_f convert photo to vangogh painting\n",
        "* discriminator_x check if the input is a vangogh Painting\n",
        "* discriminator_y check if the input is a photo\n",
        "\n",
        "In cycle consistency loss, \n",
        "* Image $X$ is passed via generator $G$ that yields generated image $\\hat{Y}$.\n",
        "* Generated image $\\hat{Y}$ is passed via generator $F$ that yields cycled image $\\hat{X}$.\n",
        "* Mean absolute error is calculated between $X$ and $\\hat{X}$.\n",
        "\n",
        "$$forward\\ cycle\\ consistency\\ loss: X -> G(X) -> F(G(X)) \\sim \\hat{X}$$\n",
        "\n",
        "$$backward\\ cycle\\ consistency\\ loss: Y -> F(Y) -> G(F(Y)) \\sim \\hat{Y}$$\n",
        "\n",
        "We multipy the cycle-consistency loss by LAMBDA afterwards.\n",
        "\n",
        "![Cycle loss](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/generative/images/cycle_loss.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NMpVGj_sW6Vo",
        "colab": {}
      },
      "source": [
        "# It is import the generator re-crates something similar to the orignial\n",
        "# therefore a multiplication (e.g 10 times) is applied.\n",
        "def calc_cycle_loss(real_image, cycled_image):\n",
        "  loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
        "  \n",
        "  return LAMBDA * loss1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "U-tJL-fX0Mq7"
      },
      "source": [
        "As shown above, generator $G$ is responsible for translating image $X$ to image $Y$. Identity loss says that, if you fed image $Y$ to generator $G$, it should yield the real image $Y$ or something close to image $Y$.\n",
        "\n",
        "$$Identity\\ loss = |G(Y) - Y| + |F(X) - X|$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "05ywEH680Aud",
        "colab": {}
      },
      "source": [
        "# to make sure the geneator_g using the real photo\n",
        "# output something similar to the real photo\n",
        "def identity_loss(real_image, same_image):\n",
        "  loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
        "  return LAMBDA * 0.5 * loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G-vjRM7IffTT"
      },
      "source": [
        "Initialize the optimizers for all the generators and the discriminators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iWCn_PVdEJZ7",
        "colab": {}
      },
      "source": [
        "# Adam is used\n",
        "generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "generator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "\n",
        "discriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aKUZnDiqQrAh"
      },
      "source": [
        "## Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WJnftd5sQsv6",
        "colab": {}
      },
      "source": [
        "# Save your model to GDrive\n",
        "#checkpoint_path = \"./checkpoints/train\"\n",
        "checkpoint_path = \"/content/drive/My Drive/Models/CircleGAN/Vangogh2Photo/checkpoints/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(generator_g=generator_g,\n",
        "                           generator_f=generator_f,\n",
        "                           discriminator_x=discriminator_x,\n",
        "                           discriminator_y=discriminator_y,\n",
        "                           generator_g_optimizer=generator_g_optimizer,\n",
        "                           generator_f_optimizer=generator_f_optimizer,\n",
        "                           discriminator_x_optimizer=discriminator_x_optimizer,\n",
        "                           discriminator_y_optimizer=discriminator_y_optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=2)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print ('Latest checkpoint restored!!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Rw1fkAczTQYh"
      },
      "source": [
        "## Training\n",
        "\n",
        "Note: This example model is trained for fewer epochs (40) than the paper (200) to keep training time reasonable for this tutorial. Predictions may be less accurate. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NS2GWywBbAWo",
        "colab": {}
      },
      "source": [
        "EPOCHS = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RmdVsmvhPxyy",
        "colab": {}
      },
      "source": [
        "def generate_images(model, test_input):\n",
        "  prediction = model(test_input)\n",
        "    \n",
        "  plt.figure(figsize=(12, 12))\n",
        "\n",
        "  display_list = [test_input[0], prediction[0]]\n",
        "  title = ['Input Image', 'Predicted Image']\n",
        "\n",
        "  for i in range(2):\n",
        "    plt.subplot(1, 2, i+1)\n",
        "    plt.title(title[i])\n",
        "    # getting the pixel values between [0, 1] to plot it.\n",
        "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kE47ERn5fyLC"
      },
      "source": [
        "Even though the training loop looks complicated, it consists of four basic steps:\n",
        "* Get the predictions.\n",
        "* Calculate the loss.\n",
        "* Calculate the gradients using backpropagation.\n",
        "* Apply the gradients to the optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrRCH7ExDHtH",
        "colab_type": "text"
      },
      "source": [
        "### GradientTape\n",
        "Reference: https://www.tensorflow.org/api_docs/python/tf/GradientTape <br>\n",
        "Record operations for automatic differentiation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOYCP4CuBoku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Simple GradientTape\n",
        "x = tf.constant(3.0)\n",
        "with tf.GradientTape() as g:\n",
        "  g.watch(x)\n",
        "  y = x**2\n",
        "dy_dx = g.gradient(y, x) # Will compute to 6.0 (dy/dx)\n",
        "print(dy_dx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNNgWv69Db4V",
        "colab_type": "text"
      },
      "source": [
        "By default, the resources held by a GradientTape are released as soon as GradientTape.gradient() method is called. To compute multiple gradients over the same computation, create a persistent gradient tape. This allows multiple calls to the gradient() method as resources are released when the tape object is garbage collected. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCPZ8KUtDn_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GradientTape with persistent\n",
        "x= tf.constant(3.0)\n",
        "with tf.GradientTape(persistent=True) as g:\n",
        "  g.watch(x)\n",
        "  y = x * x\n",
        "  z = y * y\n",
        "dz_dx = g.gradient(z, x)  # 108.0 (4*x^3 at x = 3)\n",
        "dy_dx = g.gradient(y, x)  # 6.0\n",
        "del g  # Drop the reference to the tap\n",
        "\n",
        "print(dz_dx, dy_dx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cFjd6BfSIJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The use of zip\n",
        "a = (\"John\", \"Charles\", \"Mike\")\n",
        "b = (\"Jenny\", \"Christy\", \"Monica\", \"Vicky\")\n",
        "\n",
        "x = zip(a, b)\n",
        "\n",
        "#use the tuple() function to display a readable version of the result:\n",
        "\n",
        "print(tuple(x))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KBKUV2sKXDbY",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(real_x, real_y):\n",
        "  # persistent is set to True because gen_tape and disc_tape is used more than\n",
        "  # once to calculate the gradients.\n",
        "  with tf.GradientTape(persistent=True) as gen_tape, tf.GradientTape(\n",
        "      persistent=True) as disc_tape:\n",
        "    # Generator G translates X -> Y\n",
        "    # Generator F translates Y -> X.\n",
        "    \n",
        "    # create cycled images for cycle loss.\n",
        "    fake_y = generator_g(real_x, training=True)\n",
        "    cycled_x = generator_f(fake_y, training=True)\n",
        "\n",
        "    fake_x = generator_f(real_y, training=True)\n",
        "    cycled_y = generator_g(fake_x, training=True)\n",
        "\n",
        "    # same_x and same_y are used for identity loss.\n",
        "    same_x = generator_f(real_x, training=True)\n",
        "    same_y = generator_g(real_y, training=True)\n",
        "    \n",
        "    # feed real image to disc, expect to return 1\n",
        "    disc_real_x = discriminator_x(real_x, training=True)\n",
        "    disc_real_y = discriminator_y(real_y, training=True)\n",
        "\n",
        "    # feed fake image to disc, expect to return 0\n",
        "    disc_fake_x = discriminator_x(fake_x, training=True)\n",
        "    disc_fake_y = discriminator_y(fake_y, training=True)\n",
        "\n",
        "    # calculate the adversarial loss - to see if we fooled the disc\n",
        "    gen_g_loss = generator_loss(disc_fake_y)\n",
        "    gen_f_loss = generator_loss(disc_fake_x)\n",
        "    \n",
        "    # Total generator loss = adversarial loss + cycle loss + identity loss\n",
        "    total_gen_g_loss = gen_g_loss + calc_cycle_loss(real_x, cycled_x) + identity_loss(real_x, same_x)\n",
        "    total_gen_f_loss = gen_f_loss + calc_cycle_loss(real_y, cycled_y) + identity_loss(real_y, same_y)\n",
        "\n",
        "    # Total discrimator loss = 1 for real and 0 for fake\n",
        "    disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n",
        "    disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n",
        "  \n",
        "  # Calculate the gradients for generator and discriminator\n",
        "  generator_g_gradients = gen_tape.gradient(total_gen_g_loss, \n",
        "                                            generator_g.trainable_variables)\n",
        "  generator_f_gradients = gen_tape.gradient(total_gen_f_loss, \n",
        "                                            generator_f.trainable_variables)\n",
        "  \n",
        "  discriminator_x_gradients = disc_tape.gradient(\n",
        "      disc_x_loss, discriminator_x.trainable_variables)\n",
        "  discriminator_y_gradients = disc_tape.gradient(\n",
        "      disc_y_loss, discriminator_y.trainable_variables)\n",
        "  \n",
        "  # Apply the gradients to the optimizer\n",
        "  generator_g_optimizer.apply_gradients(zip(generator_g_gradients, \n",
        "                                             generator_g.trainable_variables))\n",
        "\n",
        "  generator_f_optimizer.apply_gradients(zip(generator_f_gradients, \n",
        "                                             generator_f.trainable_variables))\n",
        "  \n",
        "  discriminator_x_optimizer.apply_gradients(\n",
        "      zip(discriminator_x_gradients,\n",
        "      discriminator_x.trainable_variables))\n",
        "  \n",
        "  discriminator_y_optimizer.apply_gradients(\n",
        "      zip(discriminator_y_gradients,\n",
        "      discriminator_y.trainable_variables))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2M7LmLtGEMQJ",
        "colab": {}
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  n = 0\n",
        "  for image_x, image_y in tf.data.Dataset.zip((train_vangogh, train_photo)):\n",
        "    train_step(image_x, image_y)\n",
        "    if n % 10 == 0:\n",
        "      print ('.', end='')\n",
        "    n+=1\n",
        "\n",
        "  clear_output(wait=True)\n",
        "  # Using a consistent image (sample_vangogh) so that the progress of the model\n",
        "  # is clearly visible.\n",
        "  generate_images(generator_g, sample_vangogh)\n",
        "\n",
        "  # Save model every 5 epoch\n",
        "  if (epoch + 1) % 10 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
        "                                                         ckpt_save_path))\n",
        "\n",
        "  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
        "                                                      time.time()-start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1RGysMU_BZhx"
      },
      "source": [
        "## Generate using test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KUgSnmy2nqSP",
        "colab": {}
      },
      "source": [
        "# Run the trained model on the test dataset\n",
        "for inp in test_vangogh.take(20):\n",
        "  generate_images(generator_g, inp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRr7l36f9o8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run the trained model on the test dataset\n",
        "for inp in test_photo.take(20):\n",
        "  generate_images(generator_f, inp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGJHPGZBjBVB",
        "colab_type": "text"
      },
      "source": [
        "## Save the image to Gdrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgeX8kEmRj0E",
        "colab_type": "text"
      },
      "source": [
        "### Function to save files to local drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzU135xwyVie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function\n",
        "def generate_images2(model, test_input, index = 0, folder_dir = \"\", prefix =\"\"):\n",
        "  prediction = model(test_input)\n",
        "    \n",
        "  fig = plt.figure(figsize=(12, 12))\n",
        "\n",
        "  display_list = [test_input[0], prediction[0]]\n",
        "  title = ['Input Image', 'Predicted Image']\n",
        "\n",
        "  for i in range(2):\n",
        "    plt.subplot(1, 2, i+1)\n",
        "    plt.title(title[i])\n",
        "    # getting the pixel values between [0, 1] to plot it.\n",
        "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
        "    plt.axis('off')\n",
        "  plt.show()\n",
        "  \n",
        "  # Save image\n",
        "  \n",
        "  output_dir = folder_dir + str(index) +\"_\"+ prefix + \"_output.png\"\n",
        "  #output_dir = F\"output/test_Monet Painting.png\"\n",
        "  print(\"Save images to:\" + output_dir)\n",
        "\n",
        "  fig.savefig(output_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdLOrduzRlw8",
        "colab_type": "text"
      },
      "source": [
        "### Create a local directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ir-qH9wOprA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# delete current output directory\n",
        "!if [ -d \"output\" ]; then rm -Rf output; fi\n",
        "\n",
        "# Create a dir in Colab env\n",
        "!mkdir -p output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ognbv_yENcE4",
        "colab_type": "text"
      },
      "source": [
        "### Run model and save results to a local directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4Az2mwwxaNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folder_dir = \"output/\"\n",
        "prefix = \"V2P_comparison\"\n",
        "\n",
        "# Run the trained model on the test dataset\n",
        "for index, inp in enumerate(test_vangogh.take(500)):\n",
        "  print(index)\n",
        "  generate_images2(generator_g, inp, index, folder_dir, prefix)\n",
        "\n",
        "# Copy the output to Google Drive\n",
        "!cp -r /content/output /content/drive/My\\ Drive/Models/CircleGAN/Vangogh2Photo/\n",
        "print(\"Outputs are saved to GDrive.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYM1n65LUp3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folder_dir = \"output/\"\n",
        "prefix = \"P2V_comparison\"\n",
        "\n",
        "# Run the trained model on the test dataset\n",
        "for index, inp in enumerate(test_photo.take(500)):\n",
        "  print(index)\n",
        "  generate_images2(generator_f, inp, index, folder_dir, prefix)\n",
        "\n",
        "# Copy the output to Google Drive\n",
        "!cp -r /content/output /content/drive/My\\ Drive/Models/CircleGAN/Vangogh2Photo/\n",
        "print(\"Outputs are saved to GDrive.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA7b9Ya0lT1I",
        "colab_type": "text"
      },
      "source": [
        "## Convert your photo to Van Gogh Style"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3rFVziOlT7G",
        "colab_type": "text"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmhvtQpleAEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Crop to the center of the image\n",
        "# https://stackoverflow.com/questions/54865717/tensorflow-crop-largest-central-square-region-of-image\n",
        "def crop_center(image):\n",
        "    h, w = image.shape[-3], image.shape[-2]\n",
        "    if h > w:\n",
        "        cropped_image = tf.image.crop_to_bounding_box(image, (h - w) // 2, 0, w, w)\n",
        "    else:\n",
        "        cropped_image = tf.image.crop_to_bounding_box(image, 0, (w - h) // 2, h, h)\n",
        "    return cropped_image\n",
        "\n",
        "# Load the image in TF\n",
        "def load_and_preprocess_image(path, center_crop = True):\n",
        "  image = tf.io.read_file(path)\n",
        "  return preprocess_image(image, center_crop)\n",
        "\n",
        "# Convert image into a normalised [256,256,3] tensor\n",
        "def preprocess_image(image, center_crop = True):\n",
        "  image = tf.image.decode_jpeg(image, channels=3)\n",
        "  \n",
        "  # crop to the center of the image\n",
        "  if center_crop: image = crop_center(image)\n",
        "        \n",
        "  # Resize the image to 256*256\n",
        "  image = tf.image.resize(image, [256, 256],\n",
        "                         method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "  \n",
        "  # normalise the image\n",
        "  image = normalize(image)\n",
        "  \n",
        "  # Expand to [1,256,256,3] tensor as input of the model\n",
        "  # Ref: https://www.tensorflow.org/api_docs/python/tf/expand_dims\n",
        "  image = tf.expand_dims(image, 0)\n",
        "  \n",
        "  return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNWp1pDHw3Sa",
        "colab_type": "text"
      },
      "source": [
        "### Load a single photo to the environment and convert it to tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fZYx4gXWxYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S08ySAY0jNMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input your image name here, including the image type.\n",
        "image_name = \"IMG_8644.jpg\" #@param "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeUh1cdIcw_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert image to tensor\n",
        "image = load_and_preprocess_image(image_name, center_crop = True)\n",
        "\n",
        "if image.shape == [1, 256, 256, 3]: print(\"The image is ready to go!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S2KM-pvwnTa",
        "colab_type": "text"
      },
      "source": [
        "### Let the model do the magic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfMpbcjxe6hv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folder_dir = \"\"\n",
        "prefix = \"YOUR_PHOTO\"\n",
        "\n",
        "# Run the trained model on the your image\n",
        "generate_images2(generator_f, image, 1, folder_dir, prefix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V23UdqWy4Ka",
        "colab_type": "text"
      },
      "source": [
        "## Convert multiple photos to Van Gogh filter\n",
        "Upload your photos to `/content` using the UI on the left handside."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1xgizBwy4UP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "files = [f for f in os.listdir('.') if re.match(r'[IMG]+.*\\.jpg', f)]\n",
        "files2 = [f for f in os.listdir('.') if re.match(r'[IMG]+.*\\.JPG', f)]\n",
        "for photos in files2:\n",
        "  files.append(photos)\n",
        "\n",
        "print(files)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFnwwA6a0miC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir output_your_photo\n",
        "folder_dir = \"output_your_photo\"\n",
        "prefix = \"Van Gough\"\n",
        "\n",
        "for index, image_name in enumerate(files):\n",
        "  # Convert image to tensor\n",
        "  image = load_and_preprocess_image(image_name, center_crop = True)\n",
        "  \n",
        "  # Run the trained model on the your image\n",
        "  generate_images2(generator_f, image, index, folder_dir, prefix)\n",
        "  \n",
        "  print(\"Outputs are saved to the Colab local environment.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ABGiHY6fE02b"
      },
      "source": [
        "\n",
        "## Next Steps\n",
        "\n",
        "This tutorial has shown how to implement CycleGAN starting from the generator and discriminator implemented in the [Pix2Pix](https://www.tensorflow.org/beta/tutorials/generative/pix2pix) tutorial. As a next step, you could try using a different dataset from [TensorFlow Datasets](https://www.tensorflow.org/datasets/datasets#cycle_gan). \n",
        "\n",
        "You could also train for a larger number of epochs to improve the results, or you could implement the modified ResNet generator used in the [paper](https://arxiv.org/abs/1703.10593) instead of the U-Net generator used here.\n",
        "\n",
        "\n",
        "\n",
        "Try using a different dataset from . You can also implement the modified ResNet generator used in the [paper](https://arxiv.org/abs/1703.10593) instead of the U-Net generator that's used here."
      ]
    }
  ]
}